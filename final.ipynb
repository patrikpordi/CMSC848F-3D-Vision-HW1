{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iporting libraries\n",
    "import torch\n",
    "from pytorch3d.renderer import (\n",
    "    AlphaCompositor,\n",
    "    RasterizationSettings,\n",
    "    MeshRenderer,\n",
    "    MeshRasterizer,\n",
    "    PointsRasterizationSettings,\n",
    "    PointsRenderer,\n",
    "    PointsRasterizer,\n",
    "    HardPhongShader,\n",
    ")\n",
    "from pytorch3d.io import load_obj\n",
    "import pytorch3d\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm.auto import tqdm\n",
    "import mcubes\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Defining functions for rendering\n",
    "def get_mesh_renderer(image_size=512, lights=None, device=None):\n",
    "    \"\"\"\n",
    "    Returns a Pytorch3D Mesh Renderer.\n",
    "\n",
    "    Args:\n",
    "        image_size (int): The rendered image size.\n",
    "        lights: A default Pytorch3D lights object.\n",
    "        device (torch.device): The torch device to use (CPU or GPU). If not specified,\n",
    "            will automatically use GPU if available, otherwise CPU.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda:0\")\n",
    "            \n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            \n",
    "    raster_settings = RasterizationSettings(\n",
    "        image_size=image_size, blur_radius=0.0, faces_per_pixel=1,\n",
    "    )\n",
    "    renderer = MeshRenderer(\n",
    "        rasterizer=MeshRasterizer(raster_settings=raster_settings),\n",
    "        shader=HardPhongShader(device=device, lights=lights),\n",
    "    )\n",
    "    return renderer\n",
    "\n",
    "# Defining unprojected depth image\n",
    "def unproject_depth_image(image, mask, depth, camera):\n",
    "    \"\"\"\n",
    "    Unprojects a depth image into a 3D point cloud.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): A square image to unproject (S, S, 3).\n",
    "        mask (torch.Tensor): A binary mask for the image (S, S).\n",
    "        depth (torch.Tensor): The depth map of the image (S, S).\n",
    "        camera: The Pytorch3D camera to render the image.\n",
    "    \n",
    "    Returns:\n",
    "        points (torch.Tensor): The 3D points of the unprojected image (N, 3).\n",
    "        rgba (torch.Tensor): The rgba color values corresponding to the unprojected\n",
    "            points (N, 4).\n",
    "    \"\"\"\n",
    "    device = camera.device\n",
    "    assert image.shape[0] == image.shape[1], \"Image must be square.\"\n",
    "    image_shape = image.shape[0]\n",
    "    ndc_pixel_coordinates = torch.linspace(1, -1, image_shape)\n",
    "    Y, X = torch.meshgrid(ndc_pixel_coordinates, ndc_pixel_coordinates)\n",
    "    xy_depth = torch.dstack([X, Y, depth])\n",
    "    points = camera.unproject_points(\n",
    "        xy_depth.to(device), in_ndc=False, from_ndc=False, world_coordinates=True,\n",
    "    )\n",
    "    points = points[mask > 0.5]\n",
    "    rgb = image[mask > 0.5]\n",
    "    rgb = rgb.to(device)\n",
    "\n",
    "    # For some reason, the Pytorch3D compositor does not apply a background color\n",
    "    # unless the pointcloud is RGBA.\n",
    "    alpha = torch.ones_like(rgb)[..., :1]\n",
    "    rgb = torch.cat([rgb, alpha], dim=1)\n",
    "\n",
    "    return points, rgb\n",
    "\n",
    "# Definining pointcloud renderer\n",
    "def get_points_renderer(\n",
    "    image_size=512, device=None, radius=0.01, background_color=(1, 1, 1)\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a Pytorch3D renderer for point clouds.\n",
    "\n",
    "    Args:\n",
    "        image_size (int): The rendered image size.\n",
    "        device (torch.device): The torch device to use (CPU or GPU). If not specified,\n",
    "            will automatically use GPU if available, otherwise CPU.\n",
    "        radius (float): The radius of the rendered point in NDC.\n",
    "        background_color (tuple): The background color of the rendered image.\n",
    "    \n",
    "    Returns:\n",
    "        PointsRenderer.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda:0\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "    raster_settings = PointsRasterizationSettings(image_size=image_size, radius=radius,)\n",
    "    renderer = PointsRenderer(\n",
    "        rasterizer=PointsRasterizer(raster_settings=raster_settings),\n",
    "        compositor=AlphaCompositor(background_color=background_color),\n",
    "    )\n",
    "    return renderer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCowRenderer:\n",
    "    \"\"\"\n",
    "    Renders a cow meshes using Pytorch3D.\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to the cow mesh.\n",
    "        image_size (int): The rendered image size.\n",
    "        color (list): The color of the rendered mesh.\n",
    "        device (torch.device): The torch device to use (CPU or GPU). If not specified,\n",
    "            will automatically use GPU if available, otherwise CPU.\n",
    "\n",
    "    Parameters:\n",
    "        renderer (MeshRenderer): The Pytorch3D mesh renderer.\n",
    "        lights (PointLights): The Pytorch3D lights object.\n",
    "        mesh (Meshes): The Pytorch3D mesh object.\n",
    "        textured_mesh (Meshes): The Pytorch3D mesh object with a texture.\n",
    "        tetra_mesh (Meshes): The Pytorch3D tetrahedron mesh object.\n",
    "        cube_mesh (Meshes): The Pytorch3D cube mesh object.\n",
    "        dolly_mesh (Meshes): The Pytorch3D mesh object for the dolly.\n",
    "        rotate_mesh (Meshes): The Pytorch3D mesh object for the rotating cow.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path=\"data/cow.obj\",image_size=512, \n",
    "                 color=[0.7, 0.7, 1],device=None):\n",
    "        # Initialize the renderer with the provided or default parameters\n",
    "        self.path = path\n",
    "        self.image_size = image_size\n",
    "        self.color = color\n",
    "        if device is None:\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = torch.device(\"cuda:0\")\n",
    "                print(\"Using CUDA\")\n",
    "            else:\n",
    "                self.device = torch.device(\"cpu\")\n",
    "                print(\"Using CPU\")\n",
    "        # Initialize the mesh renderer\n",
    "        self.renderer = self.get_mesh_renderer(image_size=self.image_size,device=self.device)\n",
    "        self.lights = pytorch3d.renderer.PointLights(location=[[0, 0, -3]], device=self.device)\n",
    "        self.cameras = None\n",
    "        self.mesh = None\n",
    "        self.textured_mesh=None\n",
    "        self.tetra_mesh=None\n",
    "        self.cube_mesh=None\n",
    "\n",
    "        # Load mesh and generate other meshes\n",
    "        self.load_mesh()\n",
    "        self.generate_tetre()\n",
    "        self.generate_cube()\n",
    "        self.dolly_mesh = pytorch3d.io.load_objs_as_meshes([\"data/cow_on_plane.obj\"])\n",
    "        self.rotate_mesh=pytorch3d.io.load_objs_as_meshes([\"data/cow_with_axis.obj\"])      \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    def get_mesh_renderer(self, image_size=512, lights=None, device=None):\n",
    "        \"\"\"\n",
    "        Returns a Pytorch3D Mesh Renderer.\n",
    "\n",
    "        Args:\n",
    "            image_size (int): The rendered image size.\n",
    "            lights: A default Pytorch3D lights object.\n",
    "            device (torch.device): The torch device to use (CPU or GPU). If not specified,\n",
    "                will automatically use GPU if available, otherwise CPU.\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            if torch.cuda.is_available():\n",
    "                device = torch.device(\"cuda:0\")\n",
    "                \n",
    "            else:\n",
    "                device = torch.device(\"cpu\")\n",
    "                \n",
    "        raster_settings = RasterizationSettings(\n",
    "            image_size=image_size, blur_radius=0.0, faces_per_pixel=1,\n",
    "        )\n",
    "        renderer = MeshRenderer(\n",
    "            rasterizer=MeshRasterizer(raster_settings=raster_settings),\n",
    "            shader=HardPhongShader(device=device, lights=lights),\n",
    "        )\n",
    "        return renderer\n",
    "    \n",
    "    def load_mesh(self):\n",
    "        # Function to load and prepare the mesh\n",
    "        vertices, faces, _ = load_obj(self.path)\n",
    "        faces = faces.verts_idx\n",
    "        vertices = vertices.unsqueeze(0)\n",
    "        faces = faces.unsqueeze(0)\n",
    "        textures = torch.ones_like(vertices)\n",
    "        textures = textures * torch.tensor(self.color)\n",
    "        mesh = pytorch3d.structures.Meshes(\n",
    "            verts=vertices,\n",
    "            faces=faces,\n",
    "            textures=pytorch3d.renderer.TexturesVertex(textures)\n",
    "        )\n",
    "        self.mesh = mesh\n",
    "\n",
    "        ## Color the mesh\n",
    "        color_1 = [0.2, 0.8, 0.2]  # Light Green\n",
    "        color_2 = [0.0, 0.0, 1]  # Blue\n",
    "\n",
    "        \n",
    "        z_coordinates = vertices[:,:, 2]\n",
    "\n",
    "        # Find the largest and smallest z-coordinates\n",
    "        largest_z = torch.max(z_coordinates)\n",
    "        smallest_z = torch.min(z_coordinates)\n",
    "\n",
    "        alpha = (z_coordinates - smallest_z) / (largest_z - smallest_z)\n",
    "        textures = torch.ones_like(vertices)  # (1, N_v, 3)\n",
    "\n",
    "        color_11=textures * torch.tensor(color_1)\n",
    "        color_22=textures * torch.tensor(color_2)\n",
    "        \n",
    "        alpha=alpha.unsqueeze(2).expand(-1, -1, 3)\n",
    "\n",
    "        colorr = alpha * color_22 + (1 - alpha) * color_11\n",
    "        \n",
    "        mesh = pytorch3d.structures.Meshes(\n",
    "            verts=vertices,\n",
    "            faces=faces,\n",
    "            textures=pytorch3d.renderer.TexturesVertex(colorr),\n",
    "        )\n",
    "        self.textured_mesh = mesh\n",
    "    \n",
    "    def generate_tetre(self):\n",
    "        # Define vertices and faces for a tetrahedron\n",
    "        vertices = torch.tensor([\n",
    "            [0.0, 0.0, 1.0],  # Vertex 0 (top)\n",
    "            [1.0, 0.0, -1.0],  # Vertex 1 (bottom front)\n",
    "            [-1.0, 0.0, -1.0],  # Vertex 2 (bottom left)\n",
    "            [0.0, 1.0, 0.0]  # Vertex 3 (bottom right)\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        faces = torch.tensor([\n",
    "            [0, 1, 2],  # Face 0\n",
    "            [0, 1, 3],  # Face 1\n",
    "            [0, 2, 3],  # Face 2\n",
    "            [1, 2, 3]   # Face 3\n",
    "        ], dtype=torch.int64)\n",
    "        # Define a single-color texture (e.g., blue)\n",
    "        \n",
    "        vertices = vertices.unsqueeze(0)\n",
    "        faces = faces.unsqueeze(0)\n",
    "        textures = torch.ones_like(vertices)\n",
    "        textures = textures * torch.tensor(self.color)\n",
    "        mesh = pytorch3d.structures.Meshes(\n",
    "            verts=vertices,\n",
    "            faces=faces,\n",
    "            textures=pytorch3d.renderer.TexturesVertex(textures)\n",
    "        )\n",
    "        self.tetra_mesh = mesh\n",
    "\n",
    "    def generate_cube(self):\n",
    "        # Define vertices and faces for a tetrahedron\n",
    "        vertices = torch.tensor([\n",
    "        [-0.5, -0.5, -0.5],  # Vertex 0\n",
    "        [-0.5, -0.5, 0.5],   # Vertex 1\n",
    "        [-0.5, 0.5, -0.5],   # Vertex 2\n",
    "        [-0.5, 0.5, 0.5],    # Vertex 3\n",
    "        [0.5, -0.5, -0.5],   # Vertex 4\n",
    "        [0.5, -0.5, 0.5],    # Vertex 5\n",
    "        [0.5, 0.5, -0.5],    # Vertex 6\n",
    "        [0.5, 0.5, 0.5]      # Vertex 7\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        faces = torch.tensor([\n",
    "            [0, 1, 3],  # Face 0\n",
    "            [0, 3, 2],  # Face 1\n",
    "            [1, 5, 7],  # Face 2\n",
    "            [1, 7, 3],  # Face 3\n",
    "            [5, 4, 6],  # Face 4\n",
    "            [5, 6, 7],  # Face 5\n",
    "            [4, 0, 2],  # Face 6\n",
    "            [4, 2, 6],  # Face 7\n",
    "            [2, 3, 7],  # Face 8\n",
    "            [2, 7, 6],  # Face 9\n",
    "            [0, 1, 5],  # Face 10\n",
    "            [0, 5, 4]   # Face 11\n",
    "        ], dtype=torch.int64)\n",
    "        # Define a single-color texture (e.g., blue)\n",
    "        \n",
    "        vertices = vertices.unsqueeze(0)\n",
    "        faces = faces.unsqueeze(0)\n",
    "        textures = torch.ones_like(vertices)\n",
    "        textures = textures * torch.tensor(self.color)\n",
    "        mesh = pytorch3d.structures.Meshes(\n",
    "            verts=vertices,\n",
    "            faces=faces,\n",
    "            textures=pytorch3d.renderer.TexturesVertex(textures)\n",
    "        )\n",
    "        self.cube_mesh = mesh\n",
    "\n",
    "    def generate_mesh(self,types=\"cow\"):\n",
    "        # Function to generate and save a mesh image\n",
    "        if types==\"cow\":\n",
    "            mesh=self.mesh.to(self.device)\n",
    "            output_file=\"results/my_cow_mesh.png\"\n",
    "        elif types==\"colored\":\n",
    "            mesh=self.textured_mesh.to(self.device)\n",
    "            output_file=\"results/textured_mesh.png\"\n",
    "        elif types==\"tetra\":\n",
    "            mesh=self.tetra_mesh.to(self.device)\n",
    "            output_file=\"results/my_tetra_mesh.png\"\n",
    "        elif types==\"cube\":\n",
    "            mesh=self.cube_mesh.to(self.device)\n",
    "            output_file=\"results/my_cube_mesh.png\"\n",
    "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "        R=torch.eye(3).unsqueeze(0),\n",
    "        T=torch.tensor([[0, 0, 3]]),\n",
    "        fov=60,device=self.device)\n",
    "        rend = self.renderer(mesh, device=self.device, cameras=cameras, lights=self.lights)\n",
    "        rend = rend.cpu().numpy()[0, ..., :3]  # (B, H, W, 4) -> (H, W, 3)\n",
    "        plt.imsave(output_file,rend)\n",
    "        \n",
    "    def generate_turntable_views(self, num_views=36,types=\"cow\"):\n",
    "        # Generate evenly spaced azimuth angles\n",
    "        azimuths = torch.linspace(0, 360, num_views)\n",
    "        if types==\"cow\":\n",
    "            mesh=self.mesh.to(self.device)\n",
    "            output_file=\"results/my_cow_turntable.gif\"\n",
    "        elif types==\"dolly\":\n",
    "            mesh  = self.dolly_mesh.to(self.device)\n",
    "            output_file=\"results/my_dolly_turntable.gif\"\n",
    "        elif types==\"colored\":\n",
    "            mesh=self.textured_mesh.to(self.device)\n",
    "            output_file=\"results/colored_turntable.png\"\n",
    "        elif types==\"tetra\":\n",
    "            mesh=self.tetra_mesh.to(self.device)\n",
    "            output_file=\"results/my_tetra_turntable.gif\"\n",
    "        elif types==\"cube\":\n",
    "            mesh=self.cube_mesh.to(self.device)\n",
    "            output_file=\"results/my_cube_turntable.gif\"\n",
    "        \n",
    "        # Fixed elevation and distance\n",
    "        elevations = torch.tensor([30.0] * num_views)  # Fixed elevation\n",
    "        distances = torch.tensor([3.0] * num_views)    # Fixed distance\n",
    "        \n",
    "        # Generate views using look_at_view_transform\n",
    "        R, T = pytorch3d.renderer.look_at_view_transform(distances, elevations, azimuths)\n",
    "        # print(R.unsqueeze(0))\n",
    "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "        R=R,\n",
    "        T=T,\n",
    "        fov=60,\n",
    "        device=self.device)\n",
    "        #Render images for each view\n",
    "        images=[]\n",
    "        for i in cameras:\n",
    "            img=(self.renderer(mesh,device=self.device, cameras=i, lights=self.lights))\n",
    "            img=img.cpu().numpy()[0, ..., :3]\n",
    "            images.append(img)\n",
    "        \n",
    "        images_pil = [Image.fromarray((img * 255).astype(np.uint8)) for img in images]\n",
    "        imageio.mimsave(output_file, images_pil, duration=30, loop=0)\n",
    "    \n",
    "    def generate_dolly(self,image_size=256,\n",
    "    num_frames=20,\n",
    "    duration=3,\n",
    "    device=None,\n",
    "    output_file=\"results/my_dolly.gif\",\n",
    "    ):\n",
    "    # Function to generate and save a dolly gif\n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda:0\")\n",
    "\n",
    "        mesh  = self.dolly_mesh.to(device)\n",
    "        \n",
    "        renderer = get_mesh_renderer(image_size=image_size, device=device)\n",
    "        lights = pytorch3d.renderer.PointLights(location=[[0.0, 0.0, -3.0]], device=device)\n",
    "\n",
    "        fovs = torch.linspace(5, 120, num_frames)\n",
    "\n",
    "        renders = []\n",
    "        for fov in tqdm(fovs):\n",
    "            distance = 5/(2*np.tan(np.radians(fov/2)))  # TODO: change this.\n",
    "            # distance = 5  # TODO: change this.\n",
    "            # T = [[0, 0, 2]]  # TODO: Change this.\n",
    "            T = [[0, 0, distance]]\n",
    "            cameras = pytorch3d.renderer.FoVPerspectiveCameras(fov=fov, T=T, device=device)\n",
    "            rend = renderer(mesh, cameras=cameras, lights=lights)\n",
    "            rend = rend[0, ..., :3].cpu().numpy()  # (N, H, W, 3)\n",
    "            renders.append(rend)\n",
    "\n",
    "        images = []\n",
    "        for i, r in enumerate(renders):\n",
    "            image = Image.fromarray((r * 255).astype(np.uint8))\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            draw.text((20, 20), f\"fov: {fovs[i]:.2f}\", fill=(255, 0, 0))\n",
    "            images.append(np.array(image))\n",
    "        imageio.mimsave(output_file, images, duration=200, loop=0)\n",
    "    \n",
    "    def rotate_cow(self):\n",
    "    # Function to rotate the cow mesh and save images\n",
    "\n",
    "        mesh  = self.rotate_mesh.to(self.device)\n",
    "        relative_transforms = [\n",
    "        {\"R_relative\": [[0, -1, 0], [1, 0, 0], [0, 0, 1]], \"T_relative\": [0, 0, 3]},\n",
    "        {\"R_relative\": [[0, 0, -1], [0, 1, 0], [1, 0, 0]], \"T_relative\": [0, 0, 3]},\n",
    "        {\"R_relative\": [[1, 0, 0], [0, 1, 0], [0, 0, 1]], \"T_relative\":  [0, 0, 6]},\n",
    "        {\"R_relative\": [[1, 0, 0], [0, 1, 0], [0, 0, 1]], \"T_relative\": [0.5, -0.5, 3]}\n",
    "    ]\n",
    "        \n",
    "\n",
    "        # Print the list of dictionaries\n",
    "        for i, transform in enumerate(relative_transforms, start=1):\n",
    "            \n",
    "            R_relative=transform[\"R_relative\"]\n",
    "            T_relative=transform[\"T_relative\"]\n",
    "            \n",
    "\n",
    "            R_relative = torch.tensor(R_relative).float()\n",
    "            T_relative = torch.tensor(T_relative).float()\n",
    "            R = R_relative @ torch.tensor([[1.0, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "            T = R_relative @ torch.tensor([0.0, 0, 0]) + T_relative\n",
    "\n",
    "            cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "            R=R.t().unsqueeze(0),\n",
    "            T=T.unsqueeze(0),\n",
    "            fov=60,device=self.device)\n",
    "            rend = self.renderer(mesh, device=self.device, cameras=cameras, lights=self.lights)\n",
    "            rend = rend.cpu().numpy()[0, ..., :3]  # (B, H, W, 4) -> (H, W, 3)\n",
    "            plt.imsave(\"results/rotate_{}.png\".format(i),rend)\n",
    "            \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "cow_renderer = MyCowRenderer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save images\n",
    "cow_renderer.generate_mesh()\n",
    "cow_renderer.generate_mesh(types=\"colored\")\n",
    "cow_renderer.generate_mesh(types=\"tetra\")\n",
    "cow_renderer.generate_mesh(types=\"cube\")\n",
    "# Generate and save gifs\n",
    "cow_renderer.generate_turntable_views(types=\"colored\")\n",
    "cow_renderer.generate_turntable_views(types=\"cow\")\n",
    "cow_renderer.generate_turntable_views(types=\"dolly\")\n",
    "cow_renderer.generate_turntable_views(types=\"tetra\")\n",
    "cow_renderer.generate_turntable_views(types=\"cube\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb83b81dc6443a1b58da3c7ea3db602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate and save dolly gif\n",
    "cow_renderer.generate_dolly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save rotating cow images\n",
    "cow_renderer.rotate_cow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPlantRenderer:\n",
    "    \"\"\"\n",
    "    Renders plant & geometric meshes using Pytorch3D.\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to the cow mesh.\n",
    "        image_size (int): The rendered image size.\n",
    "        color (list): The color of the rendered mesh.\n",
    "        device (torch.device): The torch device to use (CPU or GPU). If not specified,\n",
    "            will automatically use GPU if available, otherwise CPU.\n",
    "    Parameters:\n",
    "        renderer (MeshRenderer): The Pytorch3D mesh renderer.\n",
    "        lights (PointLights): The Pytorch3D lights object.\n",
    "        mesh (Meshes): The Pytorch3D mesh object.\n",
    "        textured_mesh (Meshes): The Pytorch3D mesh object with a texture.\n",
    "        tetra_mesh (Meshes): The Pytorch3D tetrahedron mesh object.\n",
    "        cube_mesh (Meshes): The Pytorch3D cube mesh object.\n",
    "        dolly_mesh (Meshes): The Pytorch3D mesh object for the dolly.\n",
    "        rotate_mesh (Meshes): The Pytorch3D mesh object for the rotating cow.\n",
    "    \"\"\"\n",
    "    def __init__(self, path=\"data/cow.obj\",image_size=512, \n",
    "                 color=[0.7, 0.7, 1],device=None):\n",
    "        # Constructor to initialize the class\n",
    "        # Determine the device to use (GPU or CPU)\n",
    "        # Initialize variables for point clouds and RGBA values\n",
    "        # Generate the point cloud and RGBA values\n",
    "        # Call functions to render the plant, torus, and torus mesh\n",
    "\n",
    "        if device is None:\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = torch.device(\"cuda:0\")\n",
    "                print(\"Using CUDA\")\n",
    "            else:\n",
    "                self.device = torch.device(\"cpu\")\n",
    "                print(\"Using CPU\")\n",
    "        self.points_1=None\n",
    "        self.rgba_1=None\n",
    "        self.points_2=None\n",
    "        self.rgba_2=None\n",
    "        self.points_3=None\n",
    "        self.rgba_3=None\n",
    "        self.generate_point_cloud()\n",
    "        self.points=[self.points_1,self.points_2,self.points_3]\n",
    "        self.rgba=[self.rgba_1,self.rgba_2,self.rgba_3]\n",
    "        self.render_plant(self.points,self.rgba)\n",
    "        self.render_torus()\n",
    "        self.render_torus_mesh()\n",
    "    def load_rgbd_data(self):\n",
    "        # Function to load RGBD data from a file\n",
    "        with open(\"data/rgbd_data.pkl\", \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        return data\n",
    "    def generate_point_cloud(self):\n",
    "        # Function to generate a point cloud from RGBD data\n",
    "        data_dict=self.load_rgbd_data()\n",
    "        rgb1 = data_dict['rgb1']\n",
    "        mask1 = data_dict['mask1']\n",
    "        depth1 = data_dict['depth1']\n",
    "        rgb2 = data_dict['rgb2']\n",
    "        mask2 = data_dict['mask2']\n",
    "        depth2 = data_dict['depth2']\n",
    "        cameras1 = data_dict['cameras1']\n",
    "        cameras2 = data_dict['cameras2']\n",
    "\n",
    "        image_1 = torch.tensor(rgb1, dtype=torch.float32)\n",
    "        mask_1 = torch.tensor(mask1, dtype=torch.float32)\n",
    "        depth_1 = torch.tensor(depth1, dtype=torch.float32)\n",
    "        cameras_1 = cameras1\n",
    "        image_2 = torch.tensor(rgb2, dtype=torch.float32)\n",
    "        mask_2 = torch.tensor(mask2, dtype=torch.float32)\n",
    "        depth_2 = torch.tensor(depth2, dtype=torch.float32)\n",
    "        cameras_2 = cameras2\n",
    "\n",
    "        R_np = [[-1, 0, 0], [0, -1, 0], [0, 0, 1]]\n",
    "        R_torch = torch.tensor(R_np, dtype=torch.float32)\n",
    "        points_1, rgba_1 = unproject_depth_image(image_1, mask_1, depth_1, cameras_1)\n",
    "        points_2, rgba_2 = unproject_depth_image(image_2, mask_2, depth_2, cameras_2)\n",
    "        points_1=torch.matmul(points_1,R_torch)\n",
    "        points_2=torch.matmul(points_2,R_torch)\n",
    "        points_3=torch.concat([points_1,points_2],dim=0)\n",
    "        rgba_3=torch.concat([rgba_1,rgba_2],dim=0)\n",
    "        self.points_1=points_1\n",
    "        self.rgba_1=rgba_1\n",
    "        self.points_2=points_2\n",
    "        self.rgba_2=rgba_2\n",
    "        self.points_3=points_3\n",
    "        self.rgba_3=rgba_3\n",
    "\n",
    "    def render_plant(self,\n",
    "    points,\n",
    "    rgba,\n",
    "    image_size=800,\n",
    "    background_color=(1, 1, 1),\n",
    "    device=None, num_views=36\n",
    "    ):\n",
    "        # Function to render the plant using point cloud data\n",
    "        for i in range(0,3):\n",
    "            \"\"\"\n",
    "            Renders a point cloud.\n",
    "            \"\"\"\n",
    "            if device is None:\n",
    "                if torch.cuda.is_available():\n",
    "                    device = torch.device(\"cuda:0\")\n",
    "                    \n",
    "                else:\n",
    "                    device = torch.device(\"cpu\")\n",
    "            renderer = get_points_renderer(\n",
    "                image_size=image_size, background_color=background_color\n",
    "            )\n",
    "            \n",
    "            verts = points[i].to(device).unsqueeze(0)\n",
    "            rgb = rgba[i].to(device).unsqueeze(0)\n",
    "            point_cloud = pytorch3d.structures.Pointclouds(points=verts, features=rgb)\n",
    "            R, T = pytorch3d.renderer.look_at_view_transform(4, 10, 0)\n",
    "            cameras = pytorch3d.renderer.FoVPerspectiveCameras(R=R, T=T, device=device)\n",
    "            rend = renderer(point_cloud, cameras=cameras)\n",
    "            rend = rend.cpu().numpy()[0, ..., :3]  # (B, H, W, 4) -> (H, W, 3)\n",
    "            plt.imsave(\"results/plant_{}.png\".format(i),rend)\n",
    "\n",
    "\n",
    "            # Generate evenly spaced azimuth angles\n",
    "            azimuths = torch.linspace(0, 360, num_views)\n",
    "    \n",
    "            \n",
    "            # Fixed elevation and distance\n",
    "            elevations = torch.tensor([30.0] * num_views)  # Fixed elevation\n",
    "            distances = torch.tensor([5.0] * num_views)    # Fixed distance\n",
    "            \n",
    "            # Generate views using look_at_view_transform\n",
    "            R, T = pytorch3d.renderer.look_at_view_transform(distances, elevations, azimuths)\n",
    "            # print(R.unsqueeze(0))\n",
    "            cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "            R=R,\n",
    "            T=T,\n",
    "            fov=60,\n",
    "            device=self.device)\n",
    "            #Render images for each view\n",
    "            images=[]\n",
    "            for k in cameras:\n",
    "                img=(renderer(point_cloud, cameras=k))\n",
    "                img=img.cpu().numpy()[0, ..., :3]\n",
    "                images.append(img)\n",
    "            \n",
    "            images_pil = [Image.fromarray((img * 255).astype(np.uint8)) for img in images]\n",
    "            imageio.mimsave(\"results/plant_rot_{}.gif\".format(i), images_pil, duration=30, loop=0)\n",
    "\n",
    "    def render_torus(self, image_size=256, num_samples=200, device=None,num_views=36):\n",
    "        \"\"\"\n",
    "        Renders a torus using parametric sampling. Samples num_samples ** 2 points.\n",
    "        \"\"\"\n",
    "\n",
    "        if device is None:\n",
    "            if torch.cuda.is_available():\n",
    "                device = torch.device(\"cuda:0\")\n",
    "            else:\n",
    "                device = torch.device(\"cpu\")\n",
    "\n",
    "        phi = torch.linspace(0, 2 * np.pi, num_samples)\n",
    "        theta = torch.linspace(0, 2 * np.pi, num_samples)\n",
    "        # Densely sample phi and theta on a grid\n",
    "        Phi, Theta = torch.meshgrid(phi, theta)\n",
    "        r=0.2\n",
    "        R=1\n",
    "\n",
    "        x = (R+r*torch.cos(Theta)) * torch.cos(Phi)\n",
    "        y = (R+r*torch.cos(Theta)) * torch.sin(Phi)\n",
    "        z = r*torch.sin(Theta) \n",
    "\n",
    "        points = torch.stack((x.flatten(), y.flatten(), z.flatten()), dim=1)\n",
    "        color = (points - points.min()) / (points.max() - points.min())\n",
    "\n",
    "        sphere_point_cloud = pytorch3d.structures.Pointclouds(\n",
    "            points=[points], features=[color],\n",
    "        ).to(device)\n",
    "\n",
    "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(T=[[0, 0, 3]], device=device)\n",
    "        renderer = get_points_renderer(image_size=image_size, device=device)\n",
    "        rend = renderer(sphere_point_cloud, cameras=cameras)\n",
    "        rend=rend[0, ..., :3].cpu().numpy()\n",
    "        plt.imsave(\"results/torus_para.png\",rend)\n",
    "\n",
    "        # Generate evenly spaced azimuth angles\n",
    "        azimuths = torch.linspace(0, 360, num_views)\n",
    "\n",
    "        \n",
    "        # Fixed elevation and distance\n",
    "        elevations = torch.tensor([30.0] * num_views)  # Fixed elevation\n",
    "        distances = torch.tensor([5.0] * num_views)    # Fixed distance\n",
    "        \n",
    "        # Generate views using look_at_view_transform\n",
    "        R, T = pytorch3d.renderer.look_at_view_transform(distances, elevations, azimuths)\n",
    "        # print(R.unsqueeze(0))\n",
    "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "        R=R,\n",
    "        T=T,\n",
    "        fov=60,\n",
    "        device=self.device)\n",
    "        #Render images for each view\n",
    "        images=[]\n",
    "        for k in cameras:\n",
    "            img=(renderer(sphere_point_cloud, cameras=k))\n",
    "            img=img.cpu().numpy()[0, ..., :3]\n",
    "            images.append(img)\n",
    "        \n",
    "        images_pil = [Image.fromarray((img * 255).astype(np.uint8)) for img in images]\n",
    "        imageio.mimsave(\"results/torus_para_rot.gif\", images_pil, duration=30, loop=0)\n",
    "\n",
    "\n",
    "    def render_torus_mesh(self, image_size=256, voxel_size=64, device=None, num_views=36):\n",
    "        if device is None:\n",
    "            # Function to render a torus mesh using voxelization\n",
    "            if torch.cuda.is_available():\n",
    "                device = torch.device(\"cuda:0\")\n",
    "            else:\n",
    "                device = torch.device(\"cpu\")\n",
    "\n",
    "        r=0.2\n",
    "        R=0.7\n",
    "        min_value = -1.5\n",
    "        max_value = 1.5\n",
    "        X, Y, Z = torch.meshgrid([torch.linspace(min_value, max_value, voxel_size)] * 3)\n",
    "        voxels = (np.sqrt(X ** 2 + Y ** 2) - R) ** 2 + Z ** 2 - r**2\n",
    "        vertices, faces = mcubes.marching_cubes(mcubes.smooth(voxels), isovalue=0)\n",
    "        vertices = torch.tensor(vertices).float()\n",
    "        faces = torch.tensor(faces.astype(int))\n",
    "        # Vertex coordinates are indexed by array position, so we need to\n",
    "        # renormalize the coordinate system.\n",
    "        vertices = (vertices / voxel_size) * (max_value - min_value) + min_value\n",
    "        textures = (vertices - vertices.min()) / (vertices.max() - vertices.min())\n",
    "        textures = pytorch3d.renderer.TexturesVertex(vertices.unsqueeze(0))\n",
    "\n",
    "        mesh = pytorch3d.structures.Meshes([vertices], [faces], textures=textures).to(\n",
    "            device\n",
    "        )\n",
    "        lights = pytorch3d.renderer.PointLights(location=[[0, 0.0, -4.0]], device=device,)\n",
    "        renderer = get_mesh_renderer(image_size=image_size, device=device)\n",
    "        R, T = pytorch3d.renderer.look_at_view_transform(dist=2, elev=0, azim=180)\n",
    "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(R=R, T=T, device=device)\n",
    "        rend = renderer(mesh, cameras=cameras, lights=lights)\n",
    "        rend=rend[0, ..., :3].detach().cpu().numpy().clip(0, 1)\n",
    "        plt.imsave(\"results/torus_implicit.png\",rend)\n",
    "\n",
    "        # Generate evenly spaced azimuth angles\n",
    "        azimuths = torch.linspace(0, 360, num_views)\n",
    "\n",
    "        \n",
    "        # Fixed elevation and distance\n",
    "        elevations = torch.tensor([30.0] * num_views)  # Fixed elevation\n",
    "        distances = torch.tensor([5.0] * num_views)    # Fixed distance\n",
    "        \n",
    "        # Generate views using look_at_view_transform\n",
    "        R, T = pytorch3d.renderer.look_at_view_transform(distances, elevations, azimuths)\n",
    "        # print(R.unsqueeze(0))\n",
    "        cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "        R=R,\n",
    "        T=T,\n",
    "        fov=60,\n",
    "        device=self.device)\n",
    "        #Render images for each view\n",
    "        images=[]\n",
    "        for k in cameras:\n",
    "            img=(renderer(mesh, cameras=k))\n",
    "            img=img.cpu().numpy()[0, ..., :3]\n",
    "            images.append(img)\n",
    "        \n",
    "        images_pil = [Image.fromarray((img * 255).astype(np.uint8)) for img in images]\n",
    "        imageio.mimsave(\"results/torus_implicit_rot.gif\", images_pil, duration=30, loop=0)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pordipatrik/anaconda3/envs/pytorch3d-env/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643016022/work/aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# Render the plant and the torus\n",
    "plant=MyPlantRenderer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Re-creating the Dolly Zoom (15 points)\n",
    "-------------------------------------------------------------------------------\n",
    "The [Dolly Zoom](https://en.wikipedia.org/wiki/Dolly_zoom) is a famous camera effect,\n",
    "first used in the Alfred Hitchcock film\n",
    "[Vertigo](https://www.youtube.com/watch?v=G7YJkBcRWB8).\n",
    "The core idea is to change the focal length of the camera while moving the camera in a\n",
    "way such that the subject is the same size in the frame, producing a rather unsettling\n",
    "effect.\n",
    "\n",
    "**On your webpage, include a gif with your dolly zoom effect.**\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "<img src=\"assets/dolly.gif\" alt=\"Dolly Zoom\" />\n",
    "<figcaption>Dolly Zoom effect on the cow mesh</figcaption>\n",
    "</figure>\n",
    "\n",
    "Practicing with Meshes \n",
    "===============================================================================\n",
    "\n",
    "Constructing a Tetrahedron (5 points)\n",
    "-------------------------------------------------------------------------------\n",
    "In this part, you will practice working with the geometry of 3D meshes.\n",
    "Construct a [tetrahedron mesh](https://en.wikipedia.org/wiki/Types_of_mesh#Tetrahedron) and then render it from multiple viewpoints. \n",
    "Your tetrahedron does not need to be a regular\n",
    "tetrahedron (i.e. not all faces need to be equilateral triangles) as long as it is\n",
    "obvious from the renderings that the shape is a tetrahedron.\n",
    "\n",
    "You will need to manually define the vertices and faces of the mesh. Once you have the\n",
    "vertices and faces, you can define a single-color texture, similarly to the cow in\n",
    "`render_mesh.py`. Remember that the faces are the vertex indices of the triangle mesh. \n",
    "\n",
    "It may help to draw a picture of your tetrahedron and label the vertices and assign 3D\n",
    "coordinates.\n",
    "\n",
    "**On your webpage, show a 360-degree gif animation of your tetrahedron.\n",
    "Also, list how many vertices and (triangle) faces your mesh should have.**\n",
    "\n",
    "**Answer:** The tetrahedron has **4 vertices** and **4 faces**. Notice the triangles are equilateral.\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "<img src=\"assets/360render_tetrahedron.gif\" alt=\"Tetrahedron\" />\n",
    "<figcaption>Tetrahedron mesh render and 360 degree gif</figcaption>\n",
    "</figure>\n",
    "\n",
    "Constructing a Cube (5 points)\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "Construct a cube mesh and then render it from multiple viewpoints. Remember that we are\n",
    "still working with triangle meshes, so you will need to use two sets of triangle faces\n",
    "to represent one face of the cube.\n",
    "\n",
    "**On your webpage, show a 360-degree gif animation of your cube.\n",
    "Also, list how many vertices and (triangle) faces your mesh should have.**\n",
    "\n",
    "**Answer:** The cube has **8 vertices** and **12 faces**.\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "<img src=\"assets/cube_360.gif\" alt=\"Cube\" />\n",
    "<figcaption>Cube mesh render and 360 degree gif</figcaption>\n",
    "</figure>\n",
    "\n",
    "Re-texturing a mesh (15 points)\n",
    "===============================================================================\n",
    "\n",
    "Now let's practice re-texturing a mesh. For this task, we will be retexturing the cow\n",
    "mesh such that the color smoothly changes from the front of the cow to the back of the\n",
    "cow.\n",
    "\n",
    "**In your submission, describe your choice of `color1` and `color2`, and include a gif of the\n",
    "rendered mesh.**\n",
    "\n",
    "**Answer:** I chose `color1` to be `Navy-blue (0,0,0.5)#000080` and `color2` to be `Yellow (1.0,1.0,0)#FFFF00`. The gif of the rendered mesh is shown below.\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "<img src=\"assets/retexture_360render.gif\" alt=\"Cow Color\" />\n",
    "<figcaption>Re-textured cow 360-degree rendering gif</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Parametric Functions (10 points)\n",
    "===============================================================================\n",
    "Rendering Point Clouds from RGB-D Images (15 points)\n",
    "-------------------------------------------------------------------------------\n",
    "In this part, we will practice rendering point clouds constructed from 2 RGB-D images\n",
    "from the [Common Objects in 3D Dataset](https://github.com/facebookresearch/co3d).\n",
    "\n",
    "![](assets/plant.jpg)\n",
    "\n",
    "You should use the `unproject_depth_image` function in `utils.py` to convert a depth\n",
    "image into a point cloud (parameterized as a set of 3D coordinates and corresponding\n",
    "color values). The `unproject_depth_image` function uses the camera\n",
    "intrinsics and extrinisics to cast a ray from every pixel in the image into world \n",
    "coordinates space. The ray's final distance is the depth value at that pixel, and the\n",
    "color of each point can be determined from the corresponding image pixel.\n",
    "\n",
    "Construct 3 different point clouds:\n",
    "1. The point cloud corresponding to the first image\n",
    "2. The point cloud corresponding to the second image\n",
    "3. The point cloud formed by the union of the first 2 point clouds.\n",
    "\n",
    "Try visualizing each of the point clouds from various camera viewpoints. We suggest\n",
    "starting with cameras initialized 6 units from the origin with equally spaced azimuth\n",
    "values.\n",
    "\n",
    "**In your submission, include a gif of each of these point clouds side-by-side.**\n",
    "\n",
    "**Answer:** The gif of the point clouds is shown below.\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "<img src=\"assets/PC1_360render.gif\" alt=\"Point Cloud of first image\" />\n",
    "<figcaption>360 Render of the first image's point cloud</figcaption>\n",
    "</figure>\n",
    "<figure style=\"text-align: center;\">\n",
    "<img src=\"assets/PC2_360render.gif\" alt=\"Point Cloud of first image\" />\n",
    "<figcaption>360 Render of the second image's point cloud</figcaption>\n",
    "</figure>\n",
    "<figure style=\"text-align: center;\">\n",
    "<img src=\"assets/PCUnion_360render.gif\" alt=\"Point Cloud of first image\" />\n",
    "<figcaption>360 Render of the union of the first two point clouds</figcaption>\n",
    "</figure>\n",
    "\n",
    "Parametric Functions (10 points)\n",
    "-------------------------------------------------------------------------------\n",
    "**In your writeup, include a 360-degree gif of your torus point cloud, and make sure\n",
    "the hole is visible. You may choose to texture your point cloud however you wish.**\n",
    "\n",
    "**Answer:** The gif of the torus point cloud is shown below.\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "<img src=\"assets/toroid_parametric_360render.gif\" alt=\"Torus Point Cloud\" />\n",
    "<figcaption>360 Render of the torus point cloud</figcaption>\n",
    "</figure>\n",
    "\n",
    "Implicit Surfaces (10 points)\n",
    "-------------------------------------------------------------------------------\n",
    "**In your writeup, include a 360-degree gif of your torus mesh, and make sure the hole\n",
    "is visible. In addition, discuss some of the tradeoffs between rendering as a mesh\n",
    "vs a point cloud. Things to consider might include rendering speed, rendering quality,\n",
    "ease of use, memory usage, etc.**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Part 1:**\n",
    "The gif of the torus mesh using Implicit Surfaces is shown below.\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "<img src=\"assets/toroid_implicit_360render.gif\" alt=\"Torus Mesh\" />\n",
    "<figcaption>360 Render of the torus mesh using Implicit Surfaces</figcaption>\n",
    "</figure>\n",
    "\n",
    "**Part 2:**\n",
    "The tradeoffs between rendering as a mesh vs a point cloud are as follows:\n",
    "\n",
    "Generating a point cloud with parametric functions gives several advantages. One advantage is the ease with which point clouds can be generated by simply sampling the functions in a given limit. In our case it was from 0 to 2π. During the point generation stage, memory usage is linear (O(n)), as it depends on the number of points to be stored. The quality is tied to the number of points being sampled and can be adjusted by increasing the number of points from 100 to a 1000. In my case I used 300 points. But this is a sparse representation as compared to others like meshes.\n",
    "\n",
    "While, In case of rendering surface mesh from implicit functions or i.e., signed distance functions involves constructing a voxel grid which as the name suggests would occupy cubic memory or (O(n^3)) space. The process then employs marching cubes algorithm that matches cubes to find points on the surface wherever the distance function approaches zero. The computations for all these combined is also of cubic complexity which is larger then the parametric approach. The quality of the mesh is also dependent on the resolution of the voxel grid. The higher the resolution, the better the quality of the mesh. The mesh is also a dense representation as compared to the point cloud.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
